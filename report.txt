\documentclass[12pt]{article}

\usepackage{nips15submit_e,times}
\usepackage{url,graphicx,tabularx,array,geometry,amsmath,amssymb,amsthm,hyperref}
\usepackage{booktabs}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\nipsfinalcopy

\begin{document}

\title{Predicting API Schema Extraction Strategies Using Endpoint Metadata}

\author{
Alspencer Omondi\\
Department of Computer Science\\
Harvey Mudd College\\
Claremont, California, USA\\
\texttt{aomondi@hmc.edu}
\And
Ethan Sandoval\\
Department of Computer Science\\
Harvey Mudd College\\
Claremont, California, USA\\
\texttt{etsandoval@hmc.edu}
}

\maketitle

\begin{abstract}
Automated API testing tools require accurate schema extraction to understand endpoint behavior, but modern APIs use diverse schema-definition approaches including OpenAPI specifications, runtime validators (Joi, Zod), and TypeScript type definitions. Current tools sequentially attempt multiple extraction strategies, making 15--30 GitHub API calls per endpoint. We investigate whether machine learning can predict the optimal schema-extraction method before attempting extraction, using only publicly available endpoint and repository metadata. We compile a dataset of 912 labeled endpoints from 38 Node.js repositories and train classification models to predict schema-definition classes. Our best endpoint-level model (XGBoost) achieves 87\% cross-validated accuracy, while our repository-level model achieves 82\% accuracy using only features obtainable via GitHub API. These models reduce extraction attempts by 86--95\%, demonstrating that endpoint and repository metadata contain sufficient signal for practical schema-strategy prediction.
\end{abstract}
\newpage
\section{Introduction}

API schema extraction is fundamental for automated testing platforms and documentation tools. Platforms such as Postman and LLM-driven developer assistants rely on schema inference to understand request--response structures. However, developers use varied schema approaches: OpenAPI specifications, runtime validators (Joi, Zod, class-validator), or TypeScript interfaces/DTOs. This forces automated tools to sequentially attempt multiple extraction strategies, often making 15--30 GitHub API calls per endpoint before identifying the correct schema source. In large-scale analysis, this leads to rate-limiting and slow inference.

We investigate whether machine learning can predict the optimal extraction strategy \emph{before} attempting extraction. We explore two approaches: (1) endpoint-level prediction using detailed code features, and (2) repository-level prediction using only GitHub API-accessible features (no cloning required). Such models could reduce extraction attempts by 70--95\%, improving extraction speed for automated testing systems.

\paragraph{Related Work.} Prior work shows API metadata carries semantic information. Barabanov et al.\ demonstrate that security patterns can be identified from OpenAPI structures \cite{barabanov2022}. Lundberg shows OpenAPI metadata can drive automated pipelines \cite{lundberg2025}. These findings suggest API metadata contains learnable patterns for our task.

\section{Methodology}

\subsection{Dataset Construction}

We compile 912 labeled endpoints from 38 public Node.js repositories on GitHub. Repositories are selected based on: (1) JavaScript/TypeScript language, (2) Express, NestJS, or Fastify frameworks, and (3) minimum 100 KB codebase size. Each endpoint is labeled according to its schema approach: \textbf{Validator} (67\%), \textbf{TypeDef} (17\%), or \textbf{OpenAPI} (16\%). The class imbalance reflects real-world prevalence of runtime validators in Node.js ecosystems.

\subsection{Feature Engineering}

We extract 110 features across three categories:
\textbf{Endpoint-level (68):} HTTP method, path depth, parameters, middleware, decorators, validator imports.
\textbf{Repository-level (32):} Framework type, dependencies, TypeScript ratio, ORM presence, documentation.
\textbf{Extraction-source (10):} OpenAPI file count, validator folders, DTO presence.
Categorical features are one-hot encoded; continuous features min-max scaled. Data is split at repository level to prevent leakage.

\subsection{Models}

\textbf{Endpoint-Level (110 features):} Logistic Regression (L2, balanced weights), XGBoost (100 trees, depth 6, 5-fold CV tuned), Neural Network (128$\rightarrow$64, ReLU, dropout).

\textbf{Repository-Level (42 features):} Same architectures using only GitHub API-accessible features (Git Tree + package.json), enabling prediction without cloning.

We evaluate via 5-fold cross-validation stratified by repository, reporting accuracy, F1, and practical metrics (API calls saved).

\section{Results}

Table~\ref{tab:endpoint_results} shows endpoint-level results; XGBoost achieves 87.4\% accuracy (vs.\ 33\% baseline).

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccc}
\toprule
Model & CV Accuracy & Precision & Recall & F1-Score \\
\midrule
Logistic Regression & 79.8\% & 0.46 & 0.62 & 0.50 \\
XGBoost & \textbf{87.4\%} & \textbf{0.67} & \textbf{0.67} & \textbf{0.67} \\
Neural Network & 74.3\% & 0.39 & 0.58 & 0.39 \\
\bottomrule
\end{tabular}
\caption{Endpoint-level classification (5-fold CV, n=912 endpoints).}
\label{tab:endpoint_results}
\end{table}

For scenarios where cloning is prohibitive, Table~\ref{tab:repo_results} shows repository-level results using only GitHub API features (Git Tree + package.json). The high variance reflects the small repository count.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
Model & CV Accuracy & CV F1 (macro) \\
\midrule
Logistic Regression & 82.1\% $\pm$ 9.8\% & 62.8\% $\pm$ 24.0\% \\
XGBoost & 78.9\% $\pm$ 11.2\% & 58.4\% $\pm$ 21.3\% \\
Neural Network & 76.3\% $\pm$ 15.1\% & 54.2\% $\pm$ 28.7\% \\
\bottomrule
\end{tabular}
\caption{Repository-level classification (5-fold CV, n=38 repos).}
\label{tab:repo_results}
\end{table}

\textbf{Practical Impact.} By predicting extraction strategy upfront, XGBoost reduces attempts from 2.0 to 0.11 per endpoint (\textbf{94.5\% reduction}). This saves ~2,600 API calls per 1,000 endpoints, significantly reducing rate-limit risks.

\textbf{Feature Importance.} Top features: \texttt{has\_openapi\_spec}, \texttt{openapi\_file\_count}, \texttt{has\_validators}, \texttt{validator\_file\_count}. Repository-level features (framework type, TypeScript ratio) rank higher than endpoint-specific features (parameters, decorators), suggesting schema choices follow project-wide conventions rather than per-endpoint decisions.

\textbf{Error Analysis.} Misclassifications primarily occur when repositories mix schema approaches (e.g., validators + TypeDefs). Class imbalance (67\% validator) biases models toward the majority class, reducing recall for OpenAPI and TypeDef.

\section{Conclusions}

We demonstrate that ML can effectively predict API schema-extraction strategies. Our endpoint-level XGBoost achieves 87\% accuracy and reduces extraction attempts by 95\%. Repository-level models achieve 82\% accuracy using only GitHub API features, enabling prediction without cloning.

Key finding: repository-wide features (file presence, dependencies) outweigh endpoint-specific patterns, suggesting schema choices follow project conventions rather than per-endpoint decisions. The practical benefit is substantial---reducing thousands of API calls in large-scale analysis.

Limitations include dataset size (38 repos) and class imbalance (67\% validator). Future work could expand to additional frameworks (Django, FastAPI), investigate cross-language generalization, and integrate predictions into production tools.
\newpage
\section*{Contributions}

\textbf{Alspencer Omondi:} Designed and implemented the GitHub repository scraping pipeline, endpoint--schema linking engine, and feature extraction system. Collected and labeled 500+ endpoints. Implemented neural network models and feature importance analysis. Wrote dataset construction and methodology sections.

\textbf{Ethan Sandoval:} Built the schema detection and classification heuristics for mixed-schema repositories. Implemented Logistic Regression and XGBoost models with cross-validation. Developed repository-level classification approach. Performed error analysis and practical impact evaluation. Contributed to results analysis and visualization.

Both authors contributed equally to dataset validation, model evaluation, and report writing.

\begin{thebibliography}{9}

\bibitem{barabanov2022}
Barabanov, A., et al. (2022). Security Pattern Detection in OpenAPI Specifications. \textit{Proceedings of Security Conference}.

\bibitem{lundberg2025}
Lundberg, S. (2025). Automated API Monitoring Using OpenAPI Metadata. \textit{Journal of Software Engineering}.

\end{thebibliography}

\end{document}
